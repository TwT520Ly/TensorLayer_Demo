# Demo001

## 0 简介

1. 在训练的过程中使用tensorboard，会生成logs文件夹，其中文件为`events.out.tfevents*`，在终端执行`tensorboard --logdir=logs/`，然后就可以在指定的端口上访问该平台。

## 1 mnist_example

1. 多层感知机，使用Dropout和Dense实现一个对mnist数据集的预测。
2. 训练过程


## 2 VAE

1. VAE，变分自编码器，采用逐层贪婪训练法。逐层贪婪训练方法是取得一定成功的一种方法。主要思路是每次只训练网络中的一层，首先训练得到一个只含有一个隐藏层的网络，仅当这层网络训练结束后，才会训练一个含有两层隐藏层的网络。以此类推，将训练好的前k-1层网络去训练第k层网络。最后将所有层的参数作为初始化参数，对全局网络进行微调。

2. 去噪自编码器，实现了50%的腐蚀率，如果是训练DenseLayer，只需要运行`ReconLayer.pretrain()`。如果要使用去噪自编码器，可以使用DropoutLayer作为腐蚀层。

3. 对于Sigmoid型激活函数而言，自编码器可以使用KL散度实现，KL散度也叫作相对熵，是两个概率分布P和Q的非对称式度量，用于度量基于Q(数据理论分布)的编码来编码P(数据真实分布)的样本所需的额外的位元数。对于整流器而言，对激活函数的输出进行L1正则化会使得输出变的稀疏。因此，ReconLayer只对整流激活函数提供KL散度和交叉熵两种损失度量；对于Sigmoid提供均方误差和激活输出的L1范数两种度量方式。 

## 3 堆叠去噪自编码器

## 4 卷积神经网络